# -*- coding: utf-8 -*-
"""workshop_tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDOjFuqOREW5CTSsaCTF9ndw9Gm4bWvF

## Task 1: Visualize Registration on Sample Point Clouds

This task demonstrates how to perform point cloud registration using a pretrained model and a few sample objects.

### Objective
- Load several sample point cloud.
- Load a pretrained model.
- Visualize the original, target, and registered point clouds in 3D using color-coded registration error.

### Requirements


- **Pretrained model path**:  
  `./Robust_Trained.pth`

- **Sample data path**:  
  `./sample/`  
  (Contains original `.off` mesh files like `chair_0890.off`, which are used to generate point cloud pairs during runtime.)
"""

!pip install plotly path.py torch-tps

import os
import time
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd.functional import jacobian as J
from torch.autograd import Function
import torch.utils.benchmark as benchmark
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torch.utils.tensorboard.writer import SummaryWriter
from path import Path
import glob
from scipy.spatial.distance import directed_hausdorff
from scipy.stats import bootstrap
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import copy
import random
from torch_tps import ThinPlateSpline

!unzip -q modelnet_demo.zip -d /content
path = "/content/modelnet_demo.zip"

"""## TODO: Load sample data
Make sure the following paths match your extracted folder structure
- `path = Path("./sample")`: the folder containing ModelNet `.off` sample data  


"""

# settings

# data folder
cloud = Path("modelnet_demo/saved_pointclouds") #fill in folder for saves
D, H, W = 100,100,100 #depth, height, width for discretization

#ModelNet (http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip)
path = Path("modelnet_demo/sample")  # fill in location of ModelNet

# misc
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# keypoints / graph
k = 10 #neighbours fixed
k1 = 128 #neighbours moving

#sLBP
slbp_iter = 3
slbp_cost_scale = 10 #should not be needed due to newly introduced automatic scaler
slbp_alpha = -50 #regularization

"""## Loopy Belief Propagation

Loopy Belief Propagation (sLBP): Estimate point-wise displacement using message passing on KNN graph
"""

#Loopy Belief Propagation-----------------------------------------------------------------------------------------------------------------------------------
#based on https://github.com/multimodallearning/deep-geo-reg

def pdist(x, p=2):
    if p==1:
        dist = torch.abs(x.unsqueeze(2) - x.unsqueeze(1)).sum(dim=2)
    elif p==2:
        xx = (x**2).sum(dim=2).unsqueeze(2)
        yy = xx.permute(0, 2, 1)
        dist = xx + yy - 2.0 * torch.bmm(x, x.permute(0, 2, 1))
        dist[:, torch.arange(dist.shape[1]), torch.arange(dist.shape[2])] = 0
    return dist

def pdist2(x, y, p=2):
    if p==1:
        dist = torch.abs(x.unsqueeze(2) - y.unsqueeze(1)).sum(dim=3)
    elif p==2:
        xx = (x**2).sum(dim=2).unsqueeze(2)
        yy = (y**2).sum(dim=2).unsqueeze(1)
        dist = xx + yy - 2.0 * torch.bmm(x, y.permute(0, 2, 1))
    return dist

def knn_graph(kpts, k, include_self=False):
    B, N, D = kpts.shape
    device = kpts.device

    dist = pdist(kpts)
    ind = (-dist).topk(k + (1 - int(include_self)), dim=-1)[1][:, :, 1 - int(include_self):]
    A = torch.zeros(B, N, N).to(device)
    A[:, torch.arange(N).repeat(k), ind[0].t().contiguous().view(-1)] = 1
    A[:, ind[0].t().contiguous().view(-1), torch.arange(N).repeat(k)] = 1

    return ind, dist*A, A

def lbp_graph(kpts_fixed):
    A = knn_graph(kpts_fixed, k, include_self=False)[2][0]
    edges = A.nonzero()
    edges_idx = torch.zeros_like(A).long()
    edges_idx[A.bool()] = torch.arange(edges.shape[0]).to(device)
    edges_reverse_idx = edges_idx.t()[A.bool()]
    return edges, edges_reverse_idx

def inference(kpts_fixed, kpts_moving,kpts_fixed_feat,kpts_moving_feat, f=1):
    N_p_fixed = kpts_fixed.shape[1]
    if f:
        dist = pdist2(kpts_fixed_feat, kpts_moving_feat)
    else:
        dist = pdist2(kpts_fixed, kpts_moving)
    ind = (-dist).topk(k1, dim=-1)[1]
    candidates = - kpts_fixed.view(1, N_p_fixed, 1, 3) + kpts_moving[:, ind.view(-1), :].view(1, N_p_fixed, k1, 3)
    candidates_cost = (kpts_fixed_feat.view(1, N_p_fixed, 1, -1) - kpts_moving_feat[:, ind.view(-1), :].view(1, N_p_fixed, k1, -1)).pow(2).mean(3)
    edges, edges_reverse_idx = lbp_graph(kpts_fixed)
    messages = torch.zeros((edges.shape[0], k1)).to(device)
    candidates_edges0 = candidates[0, edges[:, 0], :, :]
    candidates_edges1 = candidates[0, edges[:, 1], :, :]
    for _ in range(slbp_iter):
        temp_message = torch.zeros((N_p_fixed, k1)).to(device).scatter_add_(0, edges[:, 1].view(-1, 1).expand(-1, k1), messages)
        multi_data_cost = torch.gather(temp_message + candidates_cost.squeeze(), 0, edges[:,0].view(-1, 1).expand(-1, k1))
        reverse_messages = torch.gather(messages, 0, edges_reverse_idx.view(-1, 1).expand(-1, k1))
        multi_data_cost -= reverse_messages
        messages = torch.zeros_like(multi_data_cost)
        unroll_factor = 32
        split = torch.chunk(torch.arange(multi_data_cost.shape[0]), unroll_factor)
        for i in range(unroll_factor):
            messages[split[i]] = torch.min(multi_data_cost[split[i]].unsqueeze(1) + slbp_cost_scale*(candidates_edges0[split[i]].unsqueeze(1) - candidates_edges1[split[i]].unsqueeze(2)).pow(2).sum(3), 2)[0]
    reg_candidates_cost = (temp_message + candidates_cost.view(-1, k1)).unsqueeze(0)
    sm = F.softmax(slbp_alpha * reg_candidates_cost.view(1, N_p_fixed, -1), 2).unsqueeze(3)
    kpts_fixed_disp_pred = (candidates * sm).sum(2)
    return kpts_fixed_disp_pred



# Wrapper smooth or discrete, new candidate search in feature space or old in 3D Euclidean space

def sLBP_GF(kpts_fixed, kpts_moving, net, f=1):
    # geometric features
    kpts_fixed_feat, kpts_moving_feat = net(kpts_fixed, kpts_moving, k)
    kpts_fixed_disp_pred = inference(kpts_fixed,kpts_moving,kpts_fixed_feat,kpts_moving_feat, f)
    return kpts_fixed_disp_pred

"""## Network Architecher

EdgeConv and T-Net modules for extracting local and global geometric features from point clouds
"""

# Network Architecher -----------------------------------------------------------------------------------------------------------------------------------

class EdgeConv(nn.Module):
    #based on https://github.com/multimodallearning/deep-geo-reg
    def __init__(self, in_channels, out_channels):
        super(EdgeConv, self).__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels*2, out_channels, 1, bias=False),
            nn.InstanceNorm2d(out_channels),
            nn.LeakyReLU(),
            nn.Conv2d(out_channels, out_channels, 1, bias=False),
            nn.InstanceNorm2d(out_channels),
            nn.LeakyReLU(),
            nn.Conv2d(out_channels, out_channels, 1, bias=False),
            nn.InstanceNorm2d(out_channels),
            nn.LeakyReLU()
        )

    def forward(self, x, ind):
        B, N, D = x.shape
        k = ind.shape[2]

        y = x.reshape(B*N, D)[ind.reshape(B*N, k)].reshape(B, N, k, D)
        x = x.reshape(B, N, 1, D).expand(B, N, k, D)

        x = torch.cat([y - x, x], dim=3)

        x = self.conv(x.permute(0, 3, 1, 2))
        x = F.max_pool2d(x, (1, k))
        x = x.squeeze(3).permute(0, 2, 1)

        return x

class Tnet(nn.Module):
   #based on https://gist.github.com/nikitakaraevv/d5047c9374c2fe6c9e6251886df00cdb
   def __init__(self, k=3):
        super().__init__()
        self.k=k
        self.conv1 = nn.Conv1d(k,64,1)
        self.conv2 = nn.Conv1d(64,128,1)
        self.conv3 = nn.Conv1d(128,1024,1)
        self.fc1 = nn.Linear(1024,512)
        self.fc2 = nn.Linear(512,256)
        self.fc3 = nn.Linear(256,k*k)

        self.bn1 = nn.InstanceNorm1d(64)
        self.bn2 = nn.InstanceNorm1d(128)
        self.bn3 = nn.InstanceNorm1d(1024)
        self.bn4 = nn.InstanceNorm1d(512)
        self.bn5 = nn.InstanceNorm1d(256)

   def forward(self, input):
        # input.shape ==  bs,3,n
        bs = input.size(0)
        xb = F.relu(self.bn1(self.conv1(input)))
        xb = F.relu(self.bn2(self.conv2(xb)))
        xb = F.relu(self.bn3(self.conv3(xb)))
        pool_size = int(xb.size(-1))
        pool = F.max_pool1d(xb, pool_size).squeeze(-1)
        flat = nn.Flatten(1)(pool)
        xb = F.relu(self.bn4(self.fc1(flat)))
        xb = F.relu(self.bn5(self.fc2(xb)))
        #initialize as identity
        init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)
        if xb.is_cuda:
            init=init.cuda()
        matrix = self.fc3(xb).view(-1,self.k,self.k) + init
        return matrix

"""## Feature Descriptor Networks"""

# Feature Descriptor Networks------------------------------------------------------------------------------------------------------------

class RobustDefReg(nn.Module):
    """
    RobustDefReg
    """
    def __init__(self, D = 3):
        super().__init__()

        self.input_transform = Tnet(k=D)

        self.conv1 = EdgeConv(D, 32)
        self.conv2 = EdgeConv(32, 32)
        self.conv3 = EdgeConv(32, 64)

        self.conv4  = nn.Sequential(nn.Conv1d(64, 64, 1, bias=False),
                                    nn.InstanceNorm1d(64),
                                    nn.Conv1d(64, 64, 1))

    def forward(self, x, y, k):
        #Apply T-Net
        matrix3x3x = self.input_transform(x.transpose(1,2))
        x = torch.bmm(x, matrix3x3x)

        matrix3x3y = self.input_transform(y.transpose(1,2))
        y = torch.bmm(y, matrix3x3y)

        #Apply EdgeConv
        fixed_ind = knn_graph(x, k, include_self=True)[0]
        x = self.conv1(x, fixed_ind)
        x = self.conv2(x, fixed_ind)
        x = self.conv3(x, fixed_ind)

        moving_ind = knn_graph(y, k*3, include_self=True)[0]
        y = self.conv1(y, moving_ind)
        y = self.conv2(y, moving_ind)
        y = self.conv3(y, moving_ind)

        #Apply MLP
        x = self.conv4(x.permute(0,2,1)).permute(0,2,1)
        y = self.conv4(y.permute(0,2,1)).permute(0,2,1)

        return x, y

"""## ModelNet

`PointSampler`: Uniformly samples points from a mesh surface using triangle areas as weights.

`Normalize_ModelNet`: Normalizes point cloud by centering and scaling it to unit length.

`TPS`: Applies Thin Plate Spline deformation to simulate smooth non-rigid transformations.

`Modify`: Adds random outlier points to a point cloud.

`PointCloudData_ModelNet`: Custom PyTorch Dataset for loading and augmenting ModelNet point cloud data with deformation or outliers.
"""

# ModelNet-----------------------------------------------------------------------------------------------------------------------------------
def read_off(file):
    #https://gist.github.com/nikitakaraevv/3b95c0f39448951c431761c054dbc3fc
    if 'OFF' != file.readline().strip():
        raise('Not a valid OFF header')
    n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])
    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]
    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]
    return verts, faces

class PointSampler(object):
    #https://colab.research.google.com/github/nikitakaraevv/pointnet/blob/master/nbs/PointNetClass.ipynb
    def __init__(self, output_size):
        assert isinstance(output_size, int)
        self.output_size = output_size

    def triangle_area(self, pt1, pt2, pt3):
        side_a = np.linalg.norm(pt1 - pt2)
        side_b = np.linalg.norm(pt2 - pt3)
        side_c = np.linalg.norm(pt3 - pt1)
        s = 0.5 * ( side_a + side_b + side_c)
        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5

    def sample_point(self, pt1, pt2, pt3):
        # barycentric coordinates on a triangle
        # https://mathworld.wolfram.com/BarycentricCoordinates.html
        s, t = sorted([random.random(), random.random()])
        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]
        return (f(0), f(1), f(2))


    def __call__(self, mesh):
        verts, faces, def_lvl, typ_nr, setting, rotation = mesh
        verts = np.array(verts)
        areas = np.zeros((len(faces)))

        for i in range(len(areas)):
            areas[i] = (self.triangle_area(verts[faces[i][0]],
                                           verts[faces[i][1]],
                                           verts[faces[i][2]]))

        sampled_faces = (random.choices(faces,
                                      weights=areas,
                                      cum_weights=None,
                                      k=self.output_size))

        sampled_points = np.zeros((self.output_size, 3))

        for i in range(len(sampled_faces)):
            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],
                                                   verts[sampled_faces[i][1]],
                                                   verts[sampled_faces[i][2]]))

        return (sampled_points, def_lvl, typ_nr, setting, rotation)

class Normalize_ModelNet(object):
    def __call__(self, inp):
        pointcloud, def_lvl, typ_nr, setting, rotation = inp
        assert len(pointcloud.shape)==2

        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0)
        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))
        return  (norm_pointcloud, def_lvl, typ_nr, setting, rotation)

class TPS(object):
    """Perform Thin Plate Spline deformation"""
    def __init__(self, enabled=True, resolution = 5, alpha = 0.5):
        self.enabled = enabled
        self.tps = ThinPlateSpline(alpha)
        xs = torch.linspace(-1, 1, steps=resolution)
        ys = torch.linspace(-1, 1, steps=resolution)
        zs = torch.linspace(-1, 1, steps=resolution)
        x, y, z = torch.meshgrid(xs, ys, zs, indexing='xy')
        self.xyz = torch.stack([x, y, z], dim=3).reshape(-1, 3)

    def fit(self,pc, def_lvl):
        #noise = (torch.rand(self.xyz.shape)-0.5)*2*def_lvl #change back for uniform instead of gaussian
        self.tps.fit(self.xyz, torch.normal(self.xyz,def_lvl))#self.xyz+noise)
        transformed_pc = self.tps.transform(pc)
        return transformed_pc

    def __call__(self, inp):
        source, def_lvl, typ_nr, setting, rotation = inp
        source = torch.from_numpy(source).float()
        if not self.enabled: return (source, source.clone(), typ_nr, setting, rotation)
        target = self.fit(source, def_lvl)
        return (source, target, typ_nr, setting, rotation)

class RandRotation_z(object):
    def __init__(self, enabled=True):
        self.enabled = enabled
    def __call__(self, inp):
        if not self.enabled: return inp[0], inp[1]
        rotation = inp[2]
        return (inp[0], self.rotate(inp[1].float(), rotation))

    def rotate(self, pointcloud, rotation):
        assert len(pointcloud.shape)==2
        rot_matrix = _axis_angle_rotation('Z',torch.tensor(rotation))
        rot_pointcloud = torch.mm(rot_matrix.double(),pointcloud.double().T).T
        return  rot_pointcloud

def _axis_angle_rotation(axis: str, angle: torch.Tensor) -> torch.Tensor:
    """
    Return the rotation matrices for one of the rotations about an axis
    of which Euler angles describe, for each value of the angle given.
    https://pytorch3d.readthedocs.io/en/latest/_modules/pytorch3d/transforms/rotation_conversions.html
    Args:
        axis: Axis label "X" or "Y or "Z".
        angle: any shape tensor of Euler angles in radians
    Returns:
        Rotation matrices as tensor of shape (..., 3, 3).
    """

    cos = torch.cos(angle)
    sin = torch.sin(angle)
    one = torch.ones_like(angle)
    zero = torch.zeros_like(angle)

    if axis == "X":
        R_flat = (one, zero, zero, zero, cos, -sin, zero, sin, cos)
    elif axis == "Y":
        R_flat = (cos, zero, sin, zero, one, zero, -sin, zero, cos)
    elif axis == "Z":
        R_flat = (cos, -sin, zero, sin, cos, zero, zero, zero, one)
    else:
        raise ValueError("letter must be either X, Y or Z.")

    return torch.stack(R_flat, -1).reshape(angle.shape + (3, 3))

class Modify(object):
    """
    Add Outliers to pointcloud.
    """
    def __call__(self, inp):
        source, target, typ_nr, setting, rotation = inp
        if typ_nr == 0:
            return source, target, rotation
        elif typ_nr == 3:
            return source, self.out(target,setting), rotation

    def out(self, pointcloud, setting):
        dims = pointcloud.shape
        outliers = (torch.rand((int(dims[0]*setting/100), dims[1]))-0.5)*2
        return torch.cat((pointcloud, outliers), 0)

class PointCloudData_ModelNet(Dataset):
    def __init__(self, root_dir, transform, folder="train", typ = ["Deformation_Level"], rotation = 1/8):
        """
        Class for ModelNet dataset.

        IN:
        root_dir : str
            ModelNet folder.
        transform : transform
            Transforms to apply to data
        folder : "train" or "test"
            Load train or test data
        typ : str "Deformation_Level" or "Outlier_Data"
            Different types of challenges to process
        rotation : float in [0,1]
            Percentage of complete rotation around z-axis

        OUT:
        Dataset class object
        """

        random.seed(42)
        self.root_dir = root_dir
        # folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]
        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir) and not dir.startswith('.')]
        self.classes = {folder: i for i, folder in enumerate(folders)}
        self.transforms = transform
        self.files = []
        self.types_nr_dict= {"Deformation_Level":0, "Outlier_Data":3}
        self.settings =[[0],[0,5,15,25,35,45],[0,5,15,25,35,45],[0,5,15,25,35,45]]
        for category in self.classes.keys():
            new_dir = root_dir/Path(category)/folder
            for file in os.listdir(new_dir):
                if file.endswith('.off'):
                    if len(typ)>1:
                        typ_nrs=[]
                        for t in typ:
                            typ_nrs.append(self.types_nr_dict[t])
                        choice = np.random.randint(0,len(typ_nrs))
                        typ_nr = typ_nrs[choice]
                        typ_name = typ[choice]
                    else:
                        typ_nr = self.types_nr_dict[typ[0]]
                        typ_name = typ[0]
                    sample = {}
                    sample['pcd_path'] = new_dir/file
                    sample['category'] = category
                    sample['name'] = file
                    sample['def_lvl'] = random.randrange(1,10,1)/20 #random.randrange(1,10,1)/10 for unifrom deformation
                    sample['type'] = typ_name
                    sample['type_nr'] = typ_nr
                    sample['setting'] = self.settings[typ_nr][random.randrange(0,len(self.settings[typ_nr]),1)]
                    sample['rotation'] = np.around(random.random()*np.pi*2.*rotation,1)
                    self.files.append(sample)

    def __len__(self):
        return len(self.files)

    def __preproc__(self, file, def_lvl, typ_nr, setting, rotation):
        """
        This is a helper method that preprocesses a file.
        It reads the vertices and faces from an OFF file using the read_off() function, and applies the provided transformations (self.transforms) to the data.
        It returns the preprocessed source and target point clouds.
        """
        verts, faces = read_off(file)
        if self.transforms:
            source, target = self.transforms((verts, faces, def_lvl, typ_nr, setting, rotation))
        return source, target

    def __getitem__(self, idx):
        """
        This method is called to retrieve an item from the dataset at the given index.
        It retrieves the file path, category, name, deformation level, type, setting, and rotation for the specified index.
        Then, it opens the point cloud file using open() and passes it to the __preproc__ method for preprocessing.
        Depending on the typ_nr value, it determines the valid indices and calculates the displacement (disp).
        Finally, it returns a dictionary containing the relevant data for the item.
        """
        pcd_path = self.files[idx]['pcd_path']
        category = self.files[idx]['category']
        name = self.files[idx]['name']
        def_lvl = self.files[idx]['def_lvl']
        typ = self.files[idx]['type']
        typ_nr =  self.files[idx]['type_nr']
        setting = self.files[idx]['setting']
        rotation = self.files[idx]['rotation']
        with open(pcd_path, 'r') as f:
            source, target = self.__preproc__(f, def_lvl, typ_nr, setting, rotation)
        if typ_nr == 1:
            valid_ind=source[1]
            source = source[0]
            disp = target[valid_ind]-source
        else:
            disp = target[:source.shape[0]]-source
            valid_ind=np.arange(0,len(source),1)
        return {'source_pointcloud': source,
                'target_pointcloud': target,
                'disp': disp,
                'category': self.classes[category],
                'name': name,
                'deformation' : def_lvl,
                'type':typ,
                'setting':setting,
                'valid_ind' : valid_ind,
                'rotation' : rotation}

"""## 3D Plotting"""

#3D Plotting-----------------------------------------------------------------------------------------------------------------------------------
def visualize_rotate(data):
    #https://gist.github.com/nikitakaraevv/295f123c4f3cbecd734398eb9055fae1
    x_eye, y_eye, z_eye = 1.25, 1.25, 0.8
    frames=[]

    def rotate_z(x, y, z, theta):
        w = x+1j*y
        return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z

    for t in np.arange(0, 10.26, 0.1):
        xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)
        frames.append(dict(layout=dict(scene=dict(camera=dict(eye=dict(x=xe, y=ye, z=ze))))))
    fig = go.Figure(data=data,
                    layout=go.Layout(
                        updatemenus=[dict(type='buttons',
                                    showactive=False,
                                    y=1,
                                    x=0.8,
                                    xanchor='left',
                                    yanchor='bottom',
                                    pad=dict(t=45, r=10),
                                    buttons=[dict(label='Play',
                                                    method='animate',
                                                    args=[None, dict(frame=dict(duration=50, redraw=True),
                                                                    transition=dict(duration=0),
                                                                    fromcurrent=True,
                                                                    mode='immediate'
                                                                    )]
                                                    )
                                            ]
                                    )
                                ]
                    ),
                    frames=frames
            )

    return fig

def pcshow2(source, target, reg, err_source, err_target, err_reg, name,
            typ="demo", setting="0", rotation=0, DefLvl=0.0):
    cmax = np.max(np.concatenate([err_source, err_target, err_reg]))
    cmid = cmax / 2
    colorscale = "blackbody"

    data = [
        go.Scatter3d(x=source[:, 0], y=source[:, 1], z=source[:, 2],
                     mode='markers', marker=dict(size=5, line=dict(width=2),
                     color=err_source, showscale=True,
                     colorbar=dict(title="Euclidean Distance [pu]", titleside="right"),
                     colorscale=colorscale, cmin=0, cmid=cmid, cmax=cmax),
                     showlegend=False),
        go.Scatter3d(x=target[:, 0], y=target[:, 1], z=target[:, 2],
                     mode='markers', marker=dict(size=5, line=dict(width=2),
                     color=err_target, colorscale=colorscale, cmin=0, cmid=cmid, cmax=cmax),
                     showlegend=False),
        go.Scatter3d(x=reg[:, 0], y=reg[:, 1], z=reg[:, 2],
                     mode='markers', marker=dict(size=5, line=dict(width=2),
                     color=err_reg, colorscale=colorscale, cmin=0, cmid=cmid, cmax=cmax),
                     showlegend=False),
        go.Scatter3d(x=target[:, 0], y=target[:, 1], z=target[:, 2],
                     mode='markers', marker=dict(size=5, line=dict(width=2),
                     color="lime", opacity=0.8),
                     showlegend=False)
    ]

    fig = visualize_rotate(data)

    metric_figure = make_subplots(
        rows=1, cols=3,
        horizontal_spacing=0.05,
        subplot_titles=["a) Source", "b) Target", "c) Registered"],
        specs=[[{"type": "scene"}, {"type": "scene"}, {"type": "scene"}]]
    )

    metric_figure.append_trace(fig.data[0], row=1, col=1)
    metric_figure.append_trace(fig.data[1], row=1, col=2)
    metric_figure.append_trace(fig.data[2], row=1, col=3)

    metric_figure.update_layout(margin={"b": 0, "t": 20, "r": 0, "l": 0})
    metric_figure.show()

"""## TODO: Load the model"""

# ModelNet
train_transforms = transforms.Compose([
                    PointSampler(1024),
                    Normalize_ModelNet(),
                    TPS(),
                    Modify(),
                    RandRotation_z()
                    ])
rotation = 1/8
valid_ds = PointCloudData_ModelNet(path, folder='test', transform=train_transforms, typ=["Deformation_Level","Outlier_Data"],rotation=rotation) #["Deformation_Level","Outlier_Data"]

valid_loader = DataLoader(dataset=valid_ds, batch_size=1)

# Network
net = RobustDefReg()
net.load_state_dict(torch.load("modelnet_demo/Robust_Trained.pth", map_location=torch.device('cpu')))
net.to(device);
print(device)

"""# Visualization

## TODO: Visualize Registration
"""

deformation_level = 0.1

valid_ds = PointCloudData_ModelNet(
    path,
    folder='test',
    transform=train_transforms,
    typ=["Deformation_Level"],
    rotation=rotation
)

for sample in valid_ds.files:
    sample["def_lvl"] = deformation_level/10

valid_loader = DataLoader(dataset=valid_ds, batch_size=1)

sample_id = 4  # 0: Chair 1: Desk 2: Monitor 3: Night_stand 4: Sofa
for i, sample in enumerate(valid_loader):
    if i != sample_id:
        continue

    source = sample["source_pointcloud"].squeeze().float().to(device)
    target = sample["target_pointcloud"].squeeze().float().to(device)

    with torch.no_grad():
        disp_pred = sLBP_GF(source.unsqueeze(0), target.unsqueeze(0), net)
        registered = (source + disp_pred.squeeze()).cpu().numpy()

    source = source.cpu().numpy()
    target = target.cpu().numpy()

    from numpy.linalg import norm
    error_source = norm(target[:source.shape[0]] - source, axis=1)
    error_target = np.zeros(target.shape[0])
    error_target[:source.shape[0]] = error_source
    error_registered = norm(target[:registered.shape[0]] - registered, axis=1)

    name = f"sample_{i+1}"
    print(f"Visualizing {name}...")

    pcshow2(source, target, registered, error_source, error_target, error_registered, name)

"""# Task 2: Deformation Levels

This task explores how different levels of deformation affect point cloud registration performance.

**Objective**
- Use data with varying levels of deformation.
- Visualize the source and target point clouds under different deformation levels.
- Compare the registration results across different deformation levels.

## TODO: Set different deformation levels
"""

deformation_level = 0.25   # TODO: set different deformation levels here, such as [0.05, 0.1, 0.15 0.15, 0.25]

valid_ds = PointCloudData_ModelNet(
    path,
    folder='test',
    transform=train_transforms,
    typ=["Deformation_Level"],
    rotation=rotation
)

for sample in valid_ds.files:
    sample["def_lvl"] = (deformation_level / 10)

valid_loader = DataLoader(dataset=valid_ds, batch_size=1)

sample_id = 4  # 0: Chair 1: Desk 2: Monitor 3: Night_stand 4: Sofa
for i, sample in enumerate(valid_loader):
    if i != sample_id:
        continue

    source = sample["source_pointcloud"].squeeze().float().to(device)
    target = sample["target_pointcloud"].squeeze().float().to(device)

    with torch.no_grad():
        disp_pred = sLBP_GF(source.unsqueeze(0), target.unsqueeze(0), net)
        registered = (source + disp_pred.squeeze()).cpu().numpy()

    source = source.cpu().numpy()
    target = target.cpu().numpy()

    from numpy.linalg import norm
    error_source = norm(target[:source.shape[0]] - source, axis=1)
    error_target = np.zeros(target.shape[0])
    error_target[:source.shape[0]] = error_source
    error_registered = norm(target[:registered.shape[0]] - registered, axis=1)

    name = f"sample_{i+1}"
    print(f"Visualizing {name}...")

    pcshow2(source, target, registered, error_source, error_target, error_registered, name)

"""# Task 3: Add Outliers

This task investigates the robustness of point cloud registration under the presence of outliers.

**Objective**
- Use data that includes varying levels of outliers.
- Visualize how different outlier levels affect the point clouds.
- Run the pretrained model on these datasets.
- Compare the original and corrupted shapes to assess the impact of outliers.

## TODO: Set different outlier levels
"""

outlier_level = 35        # TODO: set different outlier levels here, such as [5, 15, 25, 35, 45]

deformation_level = 0.25

valid_ds = PointCloudData_ModelNet(
    path,
    folder='test',
    transform=train_transforms,
    typ=["Outlier_Data"],
    rotation=rotation
)

for sample in valid_ds.files:
    sample["def_lvl"] = deformation_level/10
    sample["setting"] = outlier_level

valid_loader = DataLoader(dataset=valid_ds, batch_size=1)

sample_id = 0  # 0: Chair 1: Desk 2: Monitor 3: Night_stand 4: Sofa
for i, sample in enumerate(valid_loader):
    if i != sample_id:
        continue

    source = sample["source_pointcloud"].squeeze().float().to(device)
    target = sample["target_pointcloud"].squeeze().float().to(device)

    with torch.no_grad():
        disp_pred = sLBP_GF(source.unsqueeze(0), target.unsqueeze(0), net)
        registered = (source + disp_pred.squeeze()).cpu().numpy()

    source = source.cpu().numpy()
    target = target.cpu().numpy()

    from numpy.linalg import norm
    error_source = norm(target[:source.shape[0]] - source, axis=1)
    error_target = np.zeros(target.shape[0])
    error_target[:source.shape[0]] = error_source
    error_registered = norm(target[:registered.shape[0]] - registered, axis=1)

    name = f"sample_{i+1}"
    print(f"Visualizing {name}...")

    pcshow2(source, target, registered, error_source, error_target, error_registered, name)

